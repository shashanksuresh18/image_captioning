{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os   # handling the files\n",
    "import pickle # storing numpy features\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm # how much data is process till now\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16 , preprocess_input # extract features from image data.\n",
    "from tensorflow.keras.preprocessing.image import load_img , img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Input , Dense , LSTM , Embedding , Dropout , add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16()\n",
    "\n",
    "# restructure model\n",
    "model = Model(inputs = model.inputs , outputs = model.layers[-2].output)\n",
    "\n",
    "# Summerize\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np  # Assuming you're using numpy for feature storage\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input  # Adjust as necessary\n",
    "# from your_model_library import model  # Import your model here\n",
    "\n",
    "# Set the base directory to the current working directory\n",
    "BASE_DIR = os.getcwd()\n",
    "\n",
    "# Set your images directory\n",
    "directory = os.path.join(BASE_DIR, 'Images')\n",
    "\n",
    "features = {}\n",
    "img_names = os.listdir(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process each image in the directory\n",
    "for i, img_name in enumerate(img_names):\n",
    "    print(f\"Processing {i + 1}/{len(img_names)}: {img_name}\")\n",
    "    \n",
    "    # Construct the full image path\n",
    "    img_path = os.path.join(directory, img_name)\n",
    "    \n",
    "    try:\n",
    "        # Load the image from file\n",
    "        image = load_img(img_path, target_size=(224, 224))\n",
    "        \n",
    "        # Convert image pixels to numpy array\n",
    "        image = img_to_array(image)\n",
    "        \n",
    "        # Reshape data for model\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        \n",
    "        # Preprocess image for VGG\n",
    "        image = preprocess_input(image)\n",
    "        \n",
    "        # Extract features\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        \n",
    "        # Get image ID\n",
    "        image_id = img_name.split('.')[0]\n",
    "        \n",
    "        # Store feature\n",
    "        features[image_id] = feature\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store features in pickle\n",
    "pickle.dump(features, open(os.path.join(BASE_DIR, 'features.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load features from pickle\n",
    "with open(os.path.join(BASE_DIR, 'features.pkl'), 'rb') as f:\n",
    "    features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess JSON Captions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load JSON data\n",
    "with open('captions.json', 'r') as f:\n",
    "    captions_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Visualize Sample Images with Captions\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Define the directory containing your images\n",
    "image_directory = './Images'\n",
    "\n",
    "# Specify the image IDs you want to display (replace with valid IDs from your dataset)\n",
    "image_ids_to_display = [285579, 266366, 226658, 230837]  # Example IDs\n",
    "\n",
    "# Set up the figure and axes for display\n",
    "fig, axes = plt.subplots(nrows=len(image_ids_to_display), ncols=1, figsize=(10, 15))\n",
    "\n",
    "# Loop through the specified image IDs and display the corresponding images\n",
    "for ax, image_id in zip(axes, image_ids_to_display):\n",
    "    # Find the corresponding caption\n",
    "    caption = next((item['caption'] for item in captions_data if item['image_id'] == image_id), \"Caption not found\")\n",
    "\n",
    "    # Construct the image filename according to the specified format\n",
    "    image_filename = f\"COCO_train2014_{image_id:012d}.jpg\"\n",
    "    image_path = os.path.join(image_directory, image_filename)\n",
    "\n",
    "    # Load and display the image\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"ID: {image_id}\\n{caption}\", fontsize=12)\n",
    "        ax.axis('off')  # Hide axes\n",
    "    except FileNotFoundError:\n",
    "        ax.set_title(f\"ID: {image_id}\\nImage not found\", fontsize=12)\n",
    "        ax.axis('off')  # Hide axes\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image_id(image_id):\n",
    "    \"\"\"\n",
    "    Normalize image IDs to ensure consistent format\n",
    "    \"\"\"\n",
    "    # If ID doesn't start with COCO_train2014_, add it\n",
    "    if not image_id.startswith('COCO_train2014_'):\n",
    "        # Pad the ID with leading zeros to match the format\n",
    "        padded_id = str(image_id).zfill(12)\n",
    "        return f'COCO_train2014_{padded_id}'\n",
    "    return image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_id_formats(mapping, features):\n",
    "    \"\"\"\n",
    "    Analyze and print information about ID formats in mapping and features\n",
    "    \"\"\"\n",
    "    print(\"\\nID Format Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Sample from features\n",
    "    feature_ids = list(features.keys())[:5]\n",
    "    print(\"\\nSample feature IDs:\")\n",
    "    for id_sample in feature_ids:\n",
    "        print(f\"- {id_sample}\")\n",
    "    \n",
    "    # Sample from mapping\n",
    "    mapping_ids = list(mapping.keys())[:5]\n",
    "    print(\"\\nSample mapping IDs:\")\n",
    "    for id_sample in mapping_ids:\n",
    "        print(f\"- {id_sample}\")\n",
    "    \n",
    "    # Check for mismatches\n",
    "    normalized_mapping = {}\n",
    "    id_conversions = {}\n",
    "    \n",
    "    for orig_id in mapping.keys():\n",
    "        normalized_id = normalize_image_id(orig_id)\n",
    "        if normalized_id != orig_id:\n",
    "            id_conversions[orig_id] = normalized_id\n",
    "        normalized_mapping[normalized_id] = mapping[orig_id]\n",
    "    \n",
    "    print(\"\\nID format statistics:\")\n",
    "    print(f\"Total features: {len(features)}\")\n",
    "    print(f\"Total mappings before normalization: {len(mapping)}\")\n",
    "    print(f\"Total mappings after normalization: {len(normalized_mapping)}\")\n",
    "    print(f\"Number of IDs requiring normalization: {len(id_conversions)}\")\n",
    "    \n",
    "    return normalized_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Caption with Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "for entry in tqdm(captions_data):\n",
    "    image_id = normalize_image_id(str(entry['image_id']))  # Normalize the ID here\n",
    "    caption = entry['caption']\n",
    "    if image_id not in mapping:\n",
    "        mapping[image_id] = []\n",
    "    mapping[image_id].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mapping)\n",
    "mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean captions\n",
    "import re\n",
    "def clean(mapping):\n",
    "    for key, captions in mapping.items():\n",
    "        for i in range(len(captions)):\n",
    "            caption = captions[i]\n",
    "            # Convert to lowercase\n",
    "            caption = caption.lower()\n",
    "            # Remove non-alphabet characters\n",
    "            caption = re.sub(r'[^a-z\\s]', '', caption)\n",
    "            # Remove additional spaces\n",
    "            caption = re.sub(r'\\s+', ' ', caption).strip()\n",
    "            # Add start and end tags to the caption\n",
    "            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word) > 1]) + ' endseq'\n",
    "            captions[i] = caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before preprocess of text\n",
    "mapping['COCO_train2014_000000299675']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean(mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping['COCO_train2014_000000299675']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions = []\n",
    "for key in mapping:\n",
    "    for caption in mapping[key]:\n",
    "        all_captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get maximum length of the caption available\n",
    "max_length = max(len(caption.split()) for caption in all_captions)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = list(mapping.keys())\n",
    "split = int(len(image_ids) * 0.90)\n",
    "train = image_ids[:split]\n",
    "test = image_ids[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def update_data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
    "    \"\"\"\n",
    "    Updated data generator with improved error handling and batch management\n",
    "    \"\"\"\n",
    "    # Pre-filter valid keys to avoid repeated checks\n",
    "    valid_keys = [key for key in data_keys if key in features and key in mapping]\n",
    "    print(f\"Valid training samples: {len(valid_keys)} out of {len(data_keys)}\")\n",
    "    \n",
    "    if not valid_keys:\n",
    "        raise ValueError(\"No valid training samples found!\")\n",
    "        \n",
    "    while True:\n",
    "        # Shuffle the keys for each epoch\n",
    "        np.random.shuffle(valid_keys)\n",
    "        \n",
    "        X1, X2, y = [], [], []\n",
    "        for key in valid_keys:\n",
    "            try:\n",
    "                # Get image features\n",
    "                image_feature = features[key][0]\n",
    "                \n",
    "                # Get captions for this image\n",
    "                captions = mapping[key]\n",
    "                \n",
    "                for caption in captions:\n",
    "                    # Convert caption to sequence\n",
    "                    seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                    \n",
    "                    # Create input-output pairs\n",
    "                    for i in range(1, len(seq)):\n",
    "                        # Get input and output sequences\n",
    "                        in_seq = seq[:i]\n",
    "                        out_seq = seq[i]\n",
    "                        \n",
    "                        # Pad input sequence\n",
    "                        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                        \n",
    "                        # One-hot encode output sequence\n",
    "                        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                        \n",
    "                        # Add to batch\n",
    "                        X1.append(image_feature)\n",
    "                        X2.append(in_seq)\n",
    "                        y.append(out_seq)\n",
    "                        \n",
    "                        # Yield batch when it reaches the desired size\n",
    "                        if len(X1) == batch_size:\n",
    "                            yield [np.array(X1), np.array(X2)], np.array(y)\n",
    "                            X1, X2, y = [], [], []\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing key {key}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Yield any remaining samples\n",
    "        if X1:\n",
    "            yield [np.array(X1), np.array(X2)], np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder model\n",
    "# image feature layers\n",
    "inputs1 = Input(shape=(4096,))\n",
    "fe1 = Dropout(0.4)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "# sequence feature layers\n",
    "inputs2 = Input(shape=(max_length,))\n",
    "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.4)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "# decoder model\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# plot the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
    "    while True:\n",
    "        X1, X2, y = list(), list(), list()\n",
    "        for key in data_keys:\n",
    "            # Get captions for the current image ID\n",
    "            captions = mapping[key]\n",
    "            # Process each caption\n",
    "            for caption in captions:\n",
    "                # Encode the caption sequence\n",
    "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                # Split the sequence into X, y pairs\n",
    "                for i in range(1, len(seq)):\n",
    "                    # Split into input and output pairs\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    # Pad input sequence\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                    # Encode output sequence\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    # Store the sequences, ensure features[key] exists\n",
    "                    if key in features:\n",
    "                        X1.append(features[key][0])  # Access feature vector\n",
    "                    else:\n",
    "                        print(f\"Warning: Missing features for image ID {key}\")\n",
    "                        continue  # Skip if features are missing\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "\n",
    "            # Yield data in batches\n",
    "            if len(X1) == batch_size:\n",
    "                yield [np.array(X1), np.array(X2)], np.array(y)\n",
    "                X1, X2, y = list(), list(), list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_tf_dataset(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
    "    \"\"\"\n",
    "    Creates a TensorFlow dataset for training\n",
    "    \"\"\"\n",
    "    def generator():\n",
    "        # Pre-filter valid keys\n",
    "        valid_keys = [key for key in data_keys if key in features and key in mapping]\n",
    "        np.random.shuffle(valid_keys)\n",
    "        \n",
    "        for key in valid_keys:\n",
    "            try:\n",
    "                image_feature = features[key][0]\n",
    "                captions = mapping[key]\n",
    "                \n",
    "                for caption in captions:\n",
    "                    seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                    \n",
    "                    for i in range(1, len(seq)):\n",
    "                        in_seq = seq[:i]\n",
    "                        out_seq = seq[i]\n",
    "                        \n",
    "                        # Pad sequence\n",
    "                        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                        \n",
    "                        # One-hot encode output\n",
    "                        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                        \n",
    "                        yield (image_feature, in_seq), out_seq\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing key {key}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    # Define output shapes and types\n",
    "    output_signature = (\n",
    "        (\n",
    "            tf.TensorSpec(shape=(4096,), dtype=tf.float32),  # image features\n",
    "            tf.TensorSpec(shape=(max_length,), dtype=tf.int32)  # input sequence\n",
    "        ),\n",
    "        tf.TensorSpec(shape=(vocab_size,), dtype=tf.float32)  # output sequence\n",
    "    )\n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    \n",
    "    # Batch and prefetch\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, train, mapping, features, tokenizer, max_length, vocab_size, \n",
    "                batch_size=32, epochs=20, verbose=1):\n",
    "    \"\"\"\n",
    "    Training function using TensorFlow dataset\n",
    "    \"\"\"\n",
    "    # Calculate valid samples\n",
    "    valid_samples = sum(1 for key in train if key in features and key in mapping)\n",
    "    steps = valid_samples // batch_size\n",
    "    \n",
    "    print(f\"Training Configuration:\")\n",
    "    print(f\"- Total valid samples: {valid_samples}\")\n",
    "    print(f\"- Batch size: {batch_size}\")\n",
    "    print(f\"- Steps per epoch: {steps}\")\n",
    "    print(f\"- Total epochs: {epochs}\")\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = create_tf_dataset(\n",
    "        train, mapping, features, tokenizer, \n",
    "        max_length, vocab_size, batch_size\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    history = model.fit(\n",
    "        dataset,\n",
    "        steps_per_epoch=steps,\n",
    "        epochs=epochs,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    return history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training parameters\n",
    "batch_size = 64\n",
    "epochs = 60\n",
    "\n",
    "# Train the model\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train=train,\n",
    "    mapping=mapping,\n",
    "    features=features,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_length,\n",
    "    vocab_size=vocab_size,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "# Print final training summary\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Final loss: {history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save(BASE_DIR+'/best_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.saving import load_model\n",
    "\n",
    "#Iff loading model is required\n",
    "# Load the model\n",
    "model = load_model(BASE_DIR + '/best_model.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caption Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_word(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate caption for an image\n",
    "def predict_caption(model, image, tokenizer, max_length):\n",
    "    # add start tag for generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the max length of sequence\n",
    "    for i in range(max_length):\n",
    "        # encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad the sequence\n",
    "        sequence = pad_sequences([sequence], max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([image, sequence], verbose=0)\n",
    "        # get index with high probability\n",
    "        yhat = np.argmax(yhat)\n",
    "        # convert index to word\n",
    "        word = idx_to_word(yhat, tokenizer)\n",
    "        # stop if word not found\n",
    "        if word is None:\n",
    "            break\n",
    "        # append word as input for generating next word\n",
    "        in_text += \" \" + word\n",
    "        # stop if we reach end tag\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the model (BLEU Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "# validate with test data\n",
    "actual, predicted = list(), list()\n",
    "\n",
    "for key in tqdm(test):\n",
    "    # get actual caption\n",
    "    captions = mapping[key]\n",
    "    # predict the caption for image\n",
    "    y_pred = predict_caption(model, features[key], tokenizer, max_length)\n",
    "    # split into words\n",
    "    actual_captions = [caption.split() for caption in captions]\n",
    "    y_pred = y_pred.split()\n",
    "    # append to the list\n",
    "    actual.append(actual_captions)\n",
    "    predicted.append(y_pred)\n",
    "# calcuate BLEU score\n",
    "print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_caption(image_name):\n",
    "    # Load the image\n",
    "    image_id = image_name.split('.')[0]\n",
    "    img_path = os.path.join(BASE_DIR, \"Images\", image_name)\n",
    "    image = Image.open(img_path)\n",
    "    captions = mapping[image_id]\n",
    "\n",
    "    # Display actual captions without startseq/endseq\n",
    "    print('---------------------Actual---------------------')\n",
    "    for caption in captions:\n",
    "        print(caption.replace('startseq ', '').replace(' endseq', ''))\n",
    "\n",
    "    # Predict and display the caption without startseq/endseq\n",
    "    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n",
    "    print('--------------------Predicted--------------------')\n",
    "    print(y_pred.replace('startseq ', '').replace(' endseq', ''))\n",
    "\n",
    "    # Show the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption(\"COCO_train2014_000000581766.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption(\"COCO_train2014_000000580166.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption(\"COCO_train2014_000000571357.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption(\"COCO_train2014_000000579461.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_caption(\"COCO_train2014_000000580166.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Detailed Bleu Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# validate with test data\n",
    "actual, predicted = list(), list()\n",
    "\n",
    "for key in tqdm(test):\n",
    "    # get actual caption\n",
    "    captions = mapping[key]\n",
    "    # predict the caption for image\n",
    "    y_pred = predict_caption(model, features[key], tokenizer, max_length)\n",
    "    # split into words\n",
    "    actual_captions = [caption.split() for caption in captions]\n",
    "    y_pred = y_pred.split()\n",
    "    # append to the list\n",
    "    actual.append(actual_captions)\n",
    "    predicted.append(y_pred)\n",
    "\n",
    "# Calculate BLEU scores\n",
    "bleu_1 = corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))\n",
    "bleu_2 = corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))\n",
    "bleu_3 = corpus_bleu(actual, predicted, weights=(1/3, 1/3, 1/3, 0))\n",
    "bleu_4 = corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "print(\"BLEU-1: %f\" % bleu_1)\n",
    "print(\"BLEU-2: %f\" % bleu_2)\n",
    "print(\"BLEU-3: %f\" % bleu_3)\n",
    "print(\"BLEU-4: %f\" % bleu_4)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
